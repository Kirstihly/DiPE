{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import\n",
    "Use dipe kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, cv2, torch\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image \n",
    "\n",
    "import datasets, networks\n",
    "\n",
    "from utils.utils import readlines\n",
    "from utils.kitti_utils import export_gt_depths\n",
    "from utils.evaluation_utils import *\n",
    "from layers import disp_to_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linux commands examples:\n",
    "\n",
    "python evaluate_kitti.py --data_path /home/leying/Documents/data/kitti_dataset --load_weights_folder /home/leying/Documents/DiPE/models/pt_models/dipe_eigen --eval_mono --eval_split eigen --png\n",
    "\n",
    "python evaluate_kitti.py --data_path /home/leying/Documents/data --load_weights_folder /home/leying/Documents/DiPE/models/pt_models/dipe_bench --dataset kitti_depth --eval_mono --eval_split benchmark --png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DiPE checkpoints\n",
    "# load_weights_folder = os.path.join(os.getcwd(), \"models\", \"pt_models\", \"dipe_bench\")\n",
    "load_weights_folder = os.path.join(os.getcwd(), \"models\", \"pt_models\", \"dipe_eigen\")\n",
    "# Use monodepth2 checkpoints\n",
    "# load_weights_folder = \"/home/leying/Documents/monodepth2/models/mono_640x192\"\n",
    "# load_weights_folder = \"/home/leying/Documents/monodepth2/models/mono+stereo_640x192\"\n",
    "# load_weights_folder = \"/home/leying/Documents/monodepth2/models/stereo_640x192\"\n",
    "assert os.path.isdir(load_weights_folder)\n",
    "\n",
    "# eval_split = \"benchmark\"\n",
    "eval_split = \"eigen\"\n",
    "# data_path = \"/home/leying/Documents/data\"\n",
    "data_path = \"/home/leying/Documents/data/kitti_dataset\"\n",
    "\n",
    "num_workers = 8 # Number of dataloader workers\n",
    "num_layers = 18 # Number of resnet layers, choices=[18, 34, 50, 101, 152]\n",
    "num_scales = 4\n",
    "\n",
    "debug = False\n",
    "splits_dir = os.path.join(os.getcwd(), \"splits\")\n",
    "if debug:\n",
    "    # For single image processing\n",
    "#     filenames = [\"kitti_depth/val_selection_cropped 2011_09_26_drive_0002_sync 0000000005_image_02\"] # benchmark\n",
    "    filenames = [\"2011_09_26/2011_09_26_drive_0002_sync 0000000069 l\"] # eigen\n",
    "else:\n",
    "    # For large data processing\n",
    "#     filenames = readlines(os.path.join(splits_dir, eval_split, \"val_selection_files.txt\")) # benchmark\n",
    "    filenames = readlines(os.path.join(splits_dir, eval_split, \"test_files.txt\")) # eigen\n",
    "\n",
    "\n",
    "min_depth = 0.1\n",
    "max_depth = 100.0\n",
    "pred_depth_scale_factor = 1 # If set multiplies predictions by this number\n",
    "\n",
    "# Use DiPE checkpoints\n",
    "# save_dir = os.path.join(os.getcwd(), \"results\", \"benchmark\")\n",
    "save_dir = os.path.join(os.getcwd(), \"results\", \"eigen\")\n",
    "# Use monodepth2 checkpoints\n",
    "# save_dir = \"/home/leying/Documents/monodepth2/assets/kitti_depth_val_selection_mono_640x192\"\n",
    "# save_dir = \"/home/leying/Documents/monodepth2/assets/kitti_depth_val_selection_mono+stereo_640x192\"\n",
    "# save_dir = \"/home/leying/Documents/monodepth2/assets/kitti_depth_val_selection_stereo_640x192\"\n",
    "# save_dir = \"/home/leying/Documents/monodepth2/assets/drive_test_mono_640x192\"\n",
    "# save_dir = \"/home/leying/Documents/monodepth2/assets/drive_test_mono+stereo_640x192\"\n",
    "# save_dir = \"/home/leying/Documents/monodepth2/assets/drive_test_stereo_640x192\"\n",
    "\n",
    "disable_median_scaling = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DEPTH = 1e-3\n",
    "MAX_DEPTH = 80\n",
    "\n",
    "cv2.setNumThreads(0)  # This speeds up evaluation 5x on our unix systems (OpenCV 3.3.1)\n",
    "\n",
    "datasets_dict = {\"eigen\": datasets.KITTIRAWDataset,\n",
    "                 \"eigen_benchmark\": datasets.KITTIRAWDataset,\n",
    "                 \"benchmark\": datasets.KITTIDepthTestDataset,\n",
    "                 \"odom_09\": datasets.KITTIOdomDataset,\n",
    "                 \"odom_10\": datasets.KITTIOdomDataset}\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Loading weights from /home/leying/Documents/DiPE/models/pt_models/dipe_eigen\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoints\n",
    "assert os.path.isdir(load_weights_folder), \"Cannot find a folder at {}\".format(load_weights_folder)\n",
    "print(\"-> Loading weights from {}\".format(load_weights_folder))\n",
    "\n",
    "depth_encoder_path = os.path.join(load_weights_folder, \"encoder.pth\")\n",
    "depth_decoder_path = os.path.join(load_weights_folder, \"depth.pth\")\n",
    "\n",
    "encoder_dict = torch.load(depth_encoder_path)\n",
    "\n",
    "depth_encoder = networks.ResnetEncoder(num_layers, False)\n",
    "depth_decoder = networks.DepthDecoder(depth_encoder.num_ch_enc)\n",
    "\n",
    "model_dict = depth_encoder.state_dict()\n",
    "depth_encoder.load_state_dict({k: v for k, v in encoder_dict.items() if k in model_dict})\n",
    "depth_decoder.load_state_dict(torch.load(depth_decoder_path))\n",
    "\n",
    "depth_encoder.to(device);\n",
    "depth_encoder.eval();\n",
    "depth_decoder.to(device);\n",
    "depth_decoder.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "dataset = datasets_dict[eval_split](data_path, filenames,\n",
    "                                        encoder_dict['height'], encoder_dict['width'],\n",
    "                                        [0], 4, is_train=False, img_ext='.png')\n",
    "\n",
    "dataloader = DataLoader(dataset, 16, shuffle=False, num_workers=num_workers,\n",
    "                        pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Computing predictions with size 640x192\n",
      "### AVG: 0.015598042444749311 seconds/image for 697 image(s) ###\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "print(\"-> Computing predictions with size {}x{}\".format(encoder_dict['width'], encoder_dict['height']))\n",
    "\n",
    "pred_disps = []\n",
    "times = []\n",
    "with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        input_color = data[(\"color\", 0, 0)].to(device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        output = depth_decoder(depth_encoder(input_color))\n",
    "        pred_disp, _ = disp_to_depth(output[(\"disp\", 0, 0)], min_depth, max_depth)\n",
    "        end_time = time.time() - start_time\n",
    "        if debug:\n",
    "            print(\"--- %s seconds for one image ---\" % (end_time))\n",
    "        \n",
    "        pred_disp = pred_disp.cpu()[:, 0].numpy()\n",
    "        \n",
    "        pred_disps.append(pred_disp)\n",
    "        times.append(end_time)\n",
    "        \n",
    "pred_disps = np.concatenate(pred_disps)\n",
    "mean_time = np.array(times).mean(0)\n",
    "print(\"### AVG: %s seconds/image for %s image(s) ###\" % (mean_time, len(pred_disps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> GT not found, generating...\n",
      "Exporting ground truth depths for eigen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leying/Documents/DiPE/utils/kitti_utils.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  gt_depths = np.array(gt_depths)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to eigen\n",
      "-> Evaluating\n",
      " Scaling ratios | med: 29.247262954711914 | std: 0.086000956594944\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "|   0.112  |   0.877  |   4.797  |   0.190  |   0.880  |   0.960  |   0.981  |\n",
      "\n",
      "-> Done!\n"
     ]
    }
   ],
   "source": [
    "# QA\n",
    "gt_path = os.path.join(splits_dir, eval_split, \"gt_depths.npz\")\n",
    "if not os.path.exists(gt_path):\n",
    "    print(\"-> GT not found, generating...\")\n",
    "    gt_depths = export_gt_depths(data_path, eval_split)\n",
    "else:\n",
    "    print(\"-> GT found, loading...\")\n",
    "    gt_depths = np.load(gt_path, fix_imports=True, encoding='latin1', allow_pickle=True)[\"data\"]\n",
    "    \n",
    "print(\"-> Evaluating\")\n",
    "\n",
    "errors = []\n",
    "ratios = []\n",
    "\n",
    "for i in range(pred_disps.shape[0]):\n",
    "\n",
    "    gt_depth = gt_depths[i]\n",
    "    gt_height, gt_width = gt_depth.shape[:2]\n",
    "\n",
    "    pred_disp = pred_disps[i]\n",
    "    pred_disp = cv2.resize(pred_disp, (gt_width, gt_height))\n",
    "    pred_depth = 1 / pred_disp\n",
    "\n",
    "    if eval_split == \"eigen\" or eval_split == \"eigen_benchmark\":\n",
    "        mask = np.logical_and(gt_depth > MIN_DEPTH, gt_depth < MAX_DEPTH)\n",
    "\n",
    "        crop = np.array([0.40810811 * gt_height, 0.99189189 * gt_height,\n",
    "                         0.03594771 * gt_width,  0.96405229 * gt_width]).astype(np.int32)\n",
    "        crop_mask = np.zeros(mask.shape)\n",
    "        crop_mask[crop[0]:crop[1], crop[2]:crop[3]] = 1\n",
    "        mask = np.logical_and(mask, crop_mask)\n",
    "\n",
    "    else:\n",
    "        mask = gt_depth > 0\n",
    "\n",
    "    pred_depth = pred_depth[mask]\n",
    "    gt_depth = gt_depth[mask]\n",
    "\n",
    "    pred_depth *= pred_depth_scale_factor\n",
    "    if not disable_median_scaling:\n",
    "        ratio = np.median(gt_depth) / np.median(pred_depth)\n",
    "        ratios.append(ratio)\n",
    "        pred_depth *= ratio\n",
    "\n",
    "    pred_depth[pred_depth < MIN_DEPTH] = MIN_DEPTH\n",
    "    pred_depth[pred_depth > MAX_DEPTH] = MAX_DEPTH\n",
    "\n",
    "    errors.append(compute_errors(gt_depth, pred_depth))\n",
    "\n",
    "scale_factor = pred_depth_scale_factor\n",
    "if not disable_median_scaling:\n",
    "    ratios = np.array(ratios)\n",
    "    med = np.median(ratios)\n",
    "    print(\" Scaling ratios | med: {} | std: {}\".format(med, np.std(ratios / med)))\n",
    "    scale_factor *= med\n",
    "\n",
    "mean_errors = np.array(errors).mean(0)\n",
    "\n",
    "print(\"\\n  \" + (\"{:>8} | \" * 7).format(\"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a1\", \"a2\", \"a3\"))\n",
    "print((\"|{: 8.3f}  \" * 7).format(*mean_errors.tolist()) + \"|\")\n",
    "print(\"\\n-> Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save image\n",
    "# Set scale_factor if QA is skipped\n",
    "# if not disable_median_scaling:\n",
    "#     # Scaling ratios for kitti benchmark | med: 24.854228973388672 | std: 0.06617851555347443\n",
    "#     # Scaling ratios for eigen | med: 29.51048469543457 | std: 0.07228389382362366\n",
    "#     med = 24.854228973388672\n",
    "#     scale_factor = pred_depth_scale_factor * med\n",
    "# else:\n",
    "#     scale_factor = pred_depth_scale_factor\n",
    "    \n",
    "for idx in range(len(pred_disps)):\n",
    "    disp_resized = cv2.resize(pred_disps[idx], dsize=(1216, 352))\n",
    "    depth = scale_factor / disp_resized\n",
    "    depth = np.clip(depth, 0, 80)\n",
    "    depth = np.uint16(depth * 256)\n",
    "    save_path = os.path.join(save_dir, \"{:010d}.png\".format(idx))\n",
    "    cv2.imwrite(save_path, depth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dipe",
   "language": "python",
   "name": "dipe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
